{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb09b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "import sys\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9a29253",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12656/226290586.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_pipeline_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdatapipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Users\\DARYL\\us_traffic\\src\\data_pipeline\\datapipeline.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_val_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_engineering\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess_data\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "data_pipeline_dir = os.path.abspath(\"../src/data_pipeline/\")\n",
    "sys.path.append(data_pipeline_dir)\n",
    "\n",
    "from datapipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7d61ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_path = \"../data/raw/dot_traffic_stations_2015.txt.gz\"\n",
    "traffic_df = \"../data/raw/dot_traffic_2015.txt.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1484bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df = pd.read_csv(station_path, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd669f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df = pd.read_csv(traffic_df, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81cf661",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "We take a glimpse of the data and clean both of the dataset before conducting any EDA. We will do this methodically by first cleaning the station dataset followed by the traffic dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86141955",
   "metadata": {},
   "source": [
    "## Station Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6125a43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903d72a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511c4d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f2509f",
   "metadata": {},
   "source": [
    "Based on the first 5 values of the dataset, we can already conclude that there will be missing values in some of the columns, how we deal with these missing values and columns will be based on further analysis.\n",
    "\n",
    "One thing to also note is that there is around 55 columns of data in this particular station data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ae73eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "station_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9e726b",
   "metadata": {},
   "source": [
    "I noticed that there are many columns that might be functionally the same and hence can be mapped to one another. For instance, direction_of_travel and direction_of_travel_name, will be creating some helper functions to help in obtaining mapping information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c976517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mapped_value(df: pd.DataFrame, column1: str, column2: str) -> None:\n",
    "    \"\"\"\n",
    "    Print mapped values from one column to another\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DataFrame that contains colum1 and column2\n",
    "        column1 (str): Column of the dataframe that can be mapped to column 2\n",
    "        column2 (str): Column of the dataframe that can be mapped to column1\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    column1_unique_values = df[column1].unique()\n",
    "    for value in column1_unique_values:\n",
    "        mapped_name = df.loc[\n",
    "            station_df[column1] == value][column2].unique()\n",
    "\n",
    "        if len(mapped_name) > 1:\n",
    "            print(f\"{value} maps to more than 1 value! they are {mapped_name}\")\n",
    "        else:\n",
    "            print(f\"{value} maps to {mapped_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0cea7f",
   "metadata": {},
   "source": [
    "For this initial round of cleaning, we will be mainly focusing on remapping columns that are functionally the same and dropping one of them. We will then do EDA and try to make sense of missing values to see if there is any pattern to these missing values. Dropping any columns or rows will be our **last resort**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f4bf16",
   "metadata": {},
   "source": [
    "### Algorithm of vehicle classification & Algorithm of vehicle classification name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e10fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"algorithm_of_vehicle_classification\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cbb527",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "station_df[\"algorithm_of_vehicle_classification_name\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0195f040",
   "metadata": {},
   "source": [
    "Both of these columns have null value, although something seems a bit off. Intuitively if both of these columns are similar, the missing values of these two columns should be identical. We will have to delve into this relationship deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfebc9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"algorithm_of_vehicle_classification\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d777d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"algorithm_of_vehicle_classification_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cf4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(station_df[\"algorithm_of_vehicle_classification\"].unique()) - len(station_df[\"algorithm_of_vehicle_classification_name\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c85d5c",
   "metadata": {},
   "source": [
    "Both the length of these two columns are different, which could explain why there is more missing values from one of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa4614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(station_df[\"algorithm_of_vehicle_classification\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303ff521",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(station_df[\"algorithm_of_vehicle_classification_name\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a96660",
   "metadata": {},
   "source": [
    "The algorithm_of_vehicle_classification column have two more unique values compared to the algorithm_of_vehicle_classification_name column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2fc533",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_classification_value = station_df[\"algorithm_of_vehicle_classification\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dd1408",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mapped_value(station_df, \"algorithm_of_vehicle_classification\", \"algorithm_of_vehicle_classification_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21b5baa",
   "metadata": {},
   "source": [
    "We notice that the value 0, 1 maps directly to the nan value in the algorithm_of_vehicle_classification_name column. This recouncils the difference in unique value.\n",
    "\n",
    "Since the algorithm_of_vehicle_classification column and the algorithm_of_vehicle_classification_name column are functionally the same, we will drop the algorithm_of_vehicle_classification_name column as it has more missing values and therefore less information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcad803",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.drop(\"algorithm_of_vehicle_classification_name\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456574dd",
   "metadata": {},
   "source": [
    "### Calibration of weighing system & Calibration of weighing system name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7853984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"calibration_of_weighing_system\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b201668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"calibration_of_weighing_system_name\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188dd600",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(station_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3760dea0",
   "metadata": {},
   "source": [
    "Both of these columns have null value, although something seems a bit off. similar to the algorithm_of_vehicle_column, if both of these columns are similar, the missing values of these two columns should be identical. We will have to delve into this relationship deeper.\n",
    "\n",
    "The length of this dataset is only 28466. Having approximately 71% missing data might be too much missing information to work with. We might ultimately drop these two columns but let's explore this data further first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227229e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"calibration_of_weighing_system\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f5dddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"calibration_of_weighing_system_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038947ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(station_df[\"calibration_of_weighing_system_name\"].unique()) - len(station_df[\"calibration_of_weighing_system\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3fb9cd",
   "metadata": {},
   "source": [
    "The calibration_of_weighing_system column has 3 additional values compared to the calibration_of_weighing_system_name column which could explain why it has no null values. We would have to recouncil these difference and we can use the same process as the algorithm_of_weigh_classification column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5196279",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mapped_value(station_df, \"calibration_of_weighing_system\", \"calibration_of_weighing_system_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720322cb",
   "metadata": {},
   "source": [
    "The value 0, P and 2 maps to NaN in the calibration_of_weighing_system_name column, this reconcils the difference in unique values between the two columns and we will proceed to drop the calibration_of_weighing_system_name column as it has more null values. (Although we might still drop this column in the future as mentioned earlier).\n",
    "\n",
    "Normally in situations like these, it will be important to ask the data provider for the cause of the difference but since it is not possible in this scenario, we will exercise some judgement and deal with the data in the most appropriate manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f73a1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.drop(\"calibration_of_weighing_system_name\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d708e9e0",
   "metadata": {},
   "source": [
    "### Direction of Travel & Direction of Travel Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f54ca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"direction_of_travel\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a1c4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"direction_of_travel_name\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07348848",
   "metadata": {},
   "source": [
    "Both of these columns have no null values, awesome!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68b514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"direction_of_travel\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468466b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"direction_of_travel_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aca375",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(station_df[\"direction_of_travel\"].unique()) - len(station_df[\"direction_of_travel_name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47293f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mapped_value(station_df, \"direction_of_travel\", \"direction_of_travel_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1ae005",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.drop(\"direction_of_travel\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126365d9",
   "metadata": {},
   "source": [
    "### functional_classification & functional_classification_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005e58e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(station_df[\"functional_classification\"].unique()) - len(station_df[\"functional_classification_name\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd658639",
   "metadata": {},
   "source": [
    "Both of these columns have the same number of unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ed285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mapped_value(station_df, \"functional_classification\", \"functional_classification_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7862a0",
   "metadata": {},
   "source": [
    "This shows that there is a perfect mapping of the two columns and we should probably drop one of the columns to reduce dimensionality. We will be keeping functional_classification_name as it is more informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7ff3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.drop(\"functional_classification\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6017a305",
   "metadata": {},
   "source": [
    "### lane_of_travel & lane_of_travel_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e9e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"lane_of_travel\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44739b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"lane_of_travel_name\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d21a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(station_df[\"lane_of_travel\"].unique()) - len(station_df[\"lane_of_travel_name\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b235da2",
   "metadata": {},
   "source": [
    "Even though lane_of_travel and lane_of_travel_name have no null values, they have different cardinalities, this would mean that there might be a 1 to many mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35d3590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_mapped_value(station_df, \"lane_of_travel\", \"lane_of_travel_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a68bc71",
   "metadata": {},
   "source": [
    "The relationship between lane_of_travel and lane_of_travel_name is one to many, to preserve as much information as possible, we will keep the lane_of_travel column over lane_of_travel_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6cd0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.drop(\"lane_of_travel_name\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad6679b",
   "metadata": {},
   "source": [
    "### method_of_data_retrieval & method_of_data_retrieval_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c15866",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"method_of_data_retrieval\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b1fe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"method_of_data_retrieval_name\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a308c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(station_df[\"method_of_data_retrieval\"].unique()) - len(station_df[\"method_of_data_retrieval_name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dab0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mapped_value(station_df, \"method_of_data_retrieval\", \"method_of_data_retrieval_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb65228",
   "metadata": {},
   "source": [
    "There is a perfect map, in the interest of verbosity, we will keep method_of_data_retrieval_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff23893",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.drop(\"method_of_data_retrieval_name\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a9b1a7",
   "metadata": {},
   "source": [
    "### method_of_traffic_volume_counting & method_of_traffic_volume_counting_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6e4fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"method_of_traffic_volume_counting\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57c7571",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"method_of_traffic_volume_counting_name\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4888676",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(station_df[\"method_of_traffic_volume_counting\"].unique()) - len(station_df[\"method_of_traffic_volume_counting_name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5528fcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mapped_value(station_df, \"method_of_traffic_volume_counting\", \"method_of_traffic_volume_counting_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73164610",
   "metadata": {},
   "source": [
    "We shall keep the method_of_traffic_volume_counting as it seems to have more cardinality and hence more information for our predictive model to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dc92a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.drop(\"method_of_traffic_volume_counting_name\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06869c7b",
   "metadata": {},
   "source": [
    "### method_of_truck_weighing & method_of_truck_weighing_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6056e322",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"method_of_truck_weighing\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4ed8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"method_of_truck_weighing_name\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd900983",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(station_df[\"method_of_truck_weighing\"].unique()) - len(station_df[\"method_of_truck_weighing_name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb36011",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mapped_value(station_df, \"method_of_truck_weighing\", \"method_of_truck_weighing_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373e4ccc",
   "metadata": {},
   "source": [
    "It would be preferably to keep the name of the method as it will be more informative to do EDA without the actual name, however we would have to fill in the value nan with 0 as indicated from our mapping table. We will then drop the method_of_truck_weighing column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed39164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"method_of_truck_weighing_name\"] = station_df[\"method_of_truck_weighing_name\"].fillna(\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fe82b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.drop(\"method_of_truck_weighing\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffbdeda",
   "metadata": {},
   "source": [
    "### method_of_vehicle_classification & method_of_vehicle_classification_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc3e9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"method_of_vehicle_classification\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5369b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"method_of_vehicle_classification_name\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83528ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(station_df[\"method_of_vehicle_classification\"].unique()) - len(station_df[\"method_of_vehicle_classification_name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a8aa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mapped_value(station_df, \"method_of_vehicle_classification\", \"method_of_vehicle_classification_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044b1407",
   "metadata": {},
   "source": [
    "Although we would prefer to keep the method_of_vehicle_classification_name, it seems like we will be losing information by doing so, hence we will be keeping mthe method_of_vehicle_classification instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d302089",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.drop(\"method_of_vehicle_classification_name\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19c6554",
   "metadata": {},
   "source": [
    "### primary_purpose & primary_purpose_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e368b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"primary_purpose\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1fbdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"primary_purpose_name\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135511f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(station_df[\"primary_purpose\"].unique()) - len(station_df[\"primary_purpose_name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4691be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mapped_value(station_df, \"primary_purpose\", \"primary_purpose_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11f9671",
   "metadata": {},
   "source": [
    "Since the primary_purpose column have more information than the primary_purpose_name column, we will keep that particular column instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6359ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.drop(\"primary_purpose_name\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497d4ccb",
   "metadata": {},
   "source": [
    "### sample_type_for_traffic_volume & sample_type_for_traffic_volume_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43cf4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"sample_type_for_traffic_volume\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922b7ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"sample_type_for_traffic_volume_name\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0915c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(station_df[\"sample_type_for_traffic_volume\"].unique()) - len(station_df[\"sample_type_for_traffic_volume_name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f780d785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_mapped_value(station_df, \"sample_type_for_traffic_volume\", \"sample_type_for_traffic_volume_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d43cde",
   "metadata": {},
   "source": [
    "Whether a station is used for traffic volume trends seems to be binary in nature, we can try mapping nans to the value 'Station not used for Traffic Volume Trends' while mapping the values \"Y\", \"t\" and \"T\" to 'Station used for Traffic Volume Trends', ie. 1\n",
    "\n",
    "This is what will be done however one caveat is that we are assuming this column is binary in nature. With this in mind,we can just modify the sample_type_for_traffic_volume column and consolidate all under a uniform value of \"T\" and fillna to N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6794257",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"sample_type_for_traffic_volume\"].replace(\"N\", \"0\", inplace=True)\n",
    "station_df[\"sample_type_for_traffic_volume\"].replace([\"t\", \"Y\", \"T\"], \"1\", inplace=True)\n",
    "station_df[\"sample_type_for_traffic_volume\"].fillna(\"0\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6915c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"sample_type_for_traffic_volume\"] = pd.to_numeric(station_df[\"sample_type_for_traffic_volume\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d61fc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.drop(\"sample_type_for_traffic_volume_name\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c395a2",
   "metadata": {},
   "source": [
    "### sample_type_for_truck_weight & sample_type_for_truck_weight_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11621aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"sample_type_for_truck_weight\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a273b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"sample_type_for_truck_weight_name\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1a4aa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(station_df[\"sample_type_for_truck_weight\"].unique()) - len(station_df[\"sample_type_for_truck_weight_name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b22baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mapped_value(station_df, \"sample_type_for_truck_weight\", \"sample_type_for_truck_weight_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d072eb",
   "metadata": {},
   "source": [
    "With this mapping table in mind, we will keep the sample_type_for_truck_weigh as it retains the most possible information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b12244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.drop(\"sample_type_for_truck_weight_name\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141bd3f8",
   "metadata": {},
   "source": [
    "### sample_type_for_vehicle_classification & sample_type_for_vehicle_classification_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a75860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"sample_type_for_vehicle_classification\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21dc46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"sample_type_for_vehicle_classification_name\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7abc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(station_df[\"sample_type_for_vehicle_classification\"].unique()) - len(station_df[\"sample_type_for_vehicle_classification_name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b646cf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mapped_value(station_df, \"sample_type_for_vehicle_classification\", \"sample_type_for_vehicle_classification_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5e1e54",
   "metadata": {},
   "source": [
    "Similar to the sample_type_for_traffic_volume, we will assume that this column is binary in nature, ie. a station will only either be used for Heavy Vehicle Travel Information System or it will not be used.\n",
    "\n",
    "We will map N and nans to 0 and Y, 2, T, H to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d8832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"sample_type_for_vehicle_classification\"].replace([\"H\", \"Y\", \"2\", \"T\"], \"1\",inplace=True)\n",
    "station_df[\"sample_type_for_vehicle_classification\"].replace(\"N\", \"0\",inplace=True)\n",
    "station_df[\"sample_type_for_vehicle_classification\"].fillna(\"0\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd4f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"sample_type_for_vehicle_classification\"] = pd.to_numeric(station_df[\"sample_type_for_vehicle_classification\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8255c79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.drop(\"sample_type_for_vehicle_classification_name\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7599c9",
   "metadata": {},
   "source": [
    "### type_of_sensor & type_of_sensor_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb796711",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"type_of_sensor\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26b4200",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"type_of_sensor_name\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a90ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(station_df[\"type_of_sensor\"].unique()) - len(station_df[\"type_of_sensor_name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c96307",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mapped_value(station_df, \"type_of_sensor\", \"type_of_sensor_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860369fe",
   "metadata": {},
   "source": [
    "Since there is perfect mapping, we can keep the more verbose column ie. type_of_sensor_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66d6921",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.drop(\"type_of_sensor\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d37f9e",
   "metadata": {},
   "source": [
    "We are now done with the station data (for now), moving on to the traffic data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae82dc7",
   "metadata": {},
   "source": [
    "## Traffic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81efe380",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e333cf",
   "metadata": {},
   "source": [
    "Most of the data in the traffic_df dataframe is filled up, only the restriction column seems to be missing a huge chunk of data. We will further investigate this particular column.\n",
    "\n",
    "We will also dropped the functional_classification column and direction_of_travel column since we also dropped them in the station_df from our prior analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9122b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df.drop([\"functional_classification\", \"direction_of_travel\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1f5ce4",
   "metadata": {},
   "source": [
    "### Restrictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6b82ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df[\"restrictions\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65272250",
   "metadata": {},
   "source": [
    "The entire column are just NaN values, we will proceed to drop this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a643e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df.drop(\"restrictions\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e722d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traffic_df.to_csv(\"../data/interim/dot_traffic_2015.csv\", index=False)\n",
    "# station_df.to_csv(\"../data/interim/dot_traffic_stations_2015.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126f5ad3",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "These are the columns that were dropped from the station_df based on the analysis that we have done in the notebook:\n",
    "1. algorithm_of_classification_name\n",
    "2. calibration_of_weighing_system_name\n",
    "3. direction_of_travel\n",
    "4. functional_classification\n",
    "5. lane_of_travel_name\n",
    "6. method_of_data_retrieval_name\n",
    "7. method_of_traffic_volume_counting_name\n",
    "8. method_of_truck_weighing\n",
    "9. method_of_vehicle_classification_name\n",
    "10. sample_type_for_traffic_volume_name\n",
    "11. sample_type_for_truck_weight_name\n",
    "12. sample_type_for_vehicle_classification_name\n",
    "13. type_of_sensor\n",
    "\n",
    "These are the columns that were dropped frm the traffic_df based on the analysis that was done:\n",
    "\n",
    "1. restrictions\n",
    "\n",
    "We will move on to the EDA portion, data cleaning is by no means finished, we might change the way we clean the data based on the EDA that we will be doing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73864699",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4161c03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# station_path = \"../data/interim/dot_traffic_stations_2015.csv\"\n",
    "# traffic_path = \"../data/interim/dot_traffic_2015.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bf7671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traffic_df = pd.read_csv(traffic_path)\n",
    "# station_df = pd.read_csv(station_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e7990e",
   "metadata": {},
   "source": [
    "Creating some helper functions to clean the data and for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d657c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_max_volume_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a daily max volume column for EDA purpose by summing up all the traffic_volume columns\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataframe to modify\n",
    "        \n",
    "    Returns:\n",
    "        modified_df (pd.DataFrame): DataFrame with new total_volume column\n",
    "    \"\"\"\n",
    "    volume_columns = [column for column in df.columns if column.startswith(\"traffic_volume\")]\n",
    "    df[\"total_volume\"] = df[volume_columns].sum(axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_established_year_to_int(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert the year_station_established to int by adding a 2000 to any number between 0 and 15, while adding a\n",
    "    1900 to any number between 16 and 99. Assumption is based on the dataset being from 2015 ie. it is not possible to have 2016\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataframe to modify\n",
    "        \n",
    "    Returns:\n",
    "        modified_df (pd.DataFrame): DataFrame with modified established_year column\n",
    "    \"\"\"\n",
    "    df[\"year_station_established\"] = df[\"year_station_established\"].apply(lambda x: 2000 + x if x <= 15 else 1900 + x)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_years_of_operation_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a year of operation column that computes the number of years that the observation station has been in service\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataframe to create the year of operation column\n",
    "        \n",
    "    Returns:\n",
    "        new_df (pd.DataFrame): DataFrame with a new year_of_operation column\n",
    "    \"\"\"\n",
    "    df[\"year_of_data\"] = df[\"year_of_data\"] + 2000\n",
    "    df[\"year_of_service\"] = df[\"year_of_data\"] - df[\"year_station_established\"]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d38b7e",
   "metadata": {},
   "source": [
    "Since we will be doing traffic volume prediction for New York specifically, we can drop the irrelevant stations that are not situated in New York. Based on https://www.nrcs.usda.gov/wps/portal/nrcs/detail/?cid=nrcs143_013696, the state code for New York is 36\n",
    "\n",
    "Lets do some EDA on the respective df to see if we can generate any informative insights from them. Since we are trying to predict traffic volume for the New York's rush hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaae57ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_YORK_CODE = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76a5278",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df = station_df[station_df[\"fips_state_code\"] == NEW_YORK_CODE]\n",
    "traffic_df = traffic_df[traffic_df[\"fips_state_code\"] == NEW_YORK_CODE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70e6028",
   "metadata": {},
   "source": [
    "Since all the fips_state_code will be of only one value, we can safely drop this column also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae81588",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.drop(\"fips_state_code\", inplace=True, axis=1)\n",
    "traffic_df.drop(\"fips_state_code\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77558eb7",
   "metadata": {},
   "source": [
    "After dropping selectively dropping the irrelevant stations, let's combine the two dataframe into one combined dataframe for the EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a863c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = traffic_df.merge(station_df,\n",
    "                               on=[\"station_id\", \"direction_of_travel_name\",\n",
    "                                   \"functional_classification_name\", \"lane_of_travel\",\n",
    "                                   \"year_of_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0753c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0d1c57",
   "metadata": {},
   "source": [
    "Based on a simple describe function, we can already tell there's some columns that should not be used such as method_of_data_retrieval, method_of_traffic_volume_counting as they only have unique value which will be relatively useless in helping our prediction. Will be noted down and removed later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8838891",
   "metadata": {},
   "source": [
    "It will be nice to have a total_volume for a particular day for EDA on a daily level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329028d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = create_max_volume_column(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7bc7e3",
   "metadata": {},
   "source": [
    "Lets check the common columns to make sure they agree with each other as a sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e10b5",
   "metadata": {},
   "source": [
    "Based on the basic info here, we will address missing values, and also what columns to keep for modelling before we move on to feature engineering and preprocessing the data to be used for modelling.\n",
    "\n",
    "Based on the information above, we will definitely drop the shrp_site_identification column as it has 0 (!) non-null values and there's literally nothing for us to work with.\n",
    "\n",
    "We will also do some sanity check to ensure we merged correctly ie. the number of rows for traffic_df should be exactly the same as combined_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52f5497",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_df) == len(traffic_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cef71d",
   "metadata": {},
   "source": [
    "### year_of_station_established"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b0d019",
   "metadata": {},
   "source": [
    "For easier sorting of the data, we will change the year_station_established column to int and also convert them to its full form. ie. 98 -> 1998, 0 -> 2001. We will assume that any number greater 15 is from the 1900s and any number between 0 and 15 is from the 2000s since the dataset is from 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e298b285",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = convert_established_year_to_int(combined_df)\n",
    "station_df = convert_established_year_to_int(station_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d73975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dims = (15, 11)\n",
    "fig, ax= plt.subplots(figsize=plot_dims)\n",
    "plt.title(\"Count of Year Station is Established\")\n",
    "sns.countplot(station_df[\"year_station_established\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4a18ea",
   "metadata": {},
   "source": [
    "Most of the observation stations are relatively new where they are constructed some time during the 2000s. However we have an exceeding number of stations that are built during the 1960s also. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e9d23",
   "metadata": {},
   "source": [
    "Since we will be working in time series, it will be useful to convert the date column to a datetime dtype in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4e836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"date\"] = pd.to_datetime(combined_df[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2feab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dims = (15, 11)\n",
    "fig, ax= plt.subplots(figsize=plot_dims)\n",
    "plt.title(\"Average volume per year of station is established\")\n",
    "sns.barplot(x=combined_df.groupby(\"year_station_established\").mean().index, y=\"total_volume\", \n",
    "        data=combined_df.groupby(\"year_station_established\").mean())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf66ded",
   "metadata": {},
   "source": [
    "Not much relationship can be derived from this chart although we can note that the older stations are probably kept as they are typically in high volume places which can be seen that their total volume seem to be slightly above average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d3082d",
   "metadata": {},
   "source": [
    "### Day of Week Seasonality Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9fa7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_groupby = combined_df.groupby(\"day_of_week\").sum()[\"total_volume\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e00d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dims = (15, 11)\n",
    "fig, ax= plt.subplots(figsize=plot_dims)\n",
    "plt.title(\"Volume by Day\")\n",
    "sns.barplot(x=day_groupby.index, y=day_groupby, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac18cbcb",
   "metadata": {},
   "source": [
    "We will assume for this dataset that day_7 is Sunday and day_1 is Monday, so on and so forth.\n",
    "\n",
    "Based on the traffic volume, we can see that other than Monday, all the other week days have the highest volume of traffic. This could be due to people commuting to work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca8714c",
   "metadata": {},
   "source": [
    "### Month Seasonality Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ca71fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_groupby = combined_df.groupby(\"month_of_data\").sum()[\"total_volume\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92ce33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dims = (15, 11)\n",
    "fig, ax= plt.subplots(figsize=plot_dims)\n",
    "plt.title(\"Volume by Month\")\n",
    "sns.barplot(x=month_groupby.index, y=month_groupby, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b195864",
   "metadata": {},
   "source": [
    "The volume across months seems to be fairly consistent however there seem to be a giant spike in traffic volume during the month of December. This could be due to Americans typically travelling across the country to visit their relatives during the festive seasons such as Christmas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a24e86c",
   "metadata": {},
   "source": [
    "### Hourly Seasonality Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5941008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_columns = [column for column in combined_df.columns if column.startswith(\"traffic_volume\")]\n",
    "hourly_groupby = combined_df[volume_columns].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed675cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column for clarity\n",
    "for column in volume_columns:\n",
    "    name = column.split(\"_\")[4]\n",
    "    hourly_groupby = hourly_groupby.rename(index={column: name})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336937cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dims = (15, 11)\n",
    "fig, ax= plt.subplots(figsize=plot_dims)\n",
    "plt.title(\"Volume by Hour of the Day\")\n",
    "sns.barplot(x=hourly_groupby.index, y=hourly_groupby, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33abaf8",
   "metadata": {},
   "source": [
    "Not surprisingly, odd hours such as 10pm to 5am have the lowest traffic volume among all the hours of the day. Volume starts to spike at around 6am as people start commuting to their workplace and delivery fleets start their day. Traffic volume increase continuously throughout the day until around 5pm to 6pm when it starts declining as people start to knock off and commute home where they rest and stop travelling on roads.\n",
    "\n",
    "Based on Wikipedia, rush hour is typically between 6am to 10am (morning) and 3pm to 7pm (evening) which seems to agree with this plot where traffic volume is typically at its highest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d790b4cb",
   "metadata": {},
   "source": [
    "### Lane of Travel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3beff3",
   "metadata": {},
   "source": [
    "Lane of travel is in int dtype when it is actually a mapping, converting it back to category for visualisation purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf8c59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"lane_of_travel\"] = combined_df[\"lane_of_travel\"].astype(\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1610815",
   "metadata": {},
   "outputs": [],
   "source": [
    "lane_of_travel_groupby = combined_df.groupby(\"lane_of_travel\").sum()\n",
    "lane_of_travel_groupby.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cd0e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dims = (15, 11)\n",
    "fig, ax= plt.subplots(figsize=plot_dims)\n",
    "plt.title(\"Volume by lane\")\n",
    "sns.barplot(y=\"total_volume\", x=\"lane_of_travel\", data=lane_of_travel_groupby)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f097ce6",
   "metadata": {},
   "source": [
    "Based on the prior mapping, lane_of_travel value of 1 is the right (outer most lane) while lane 2 to 5 are \"Other Lanes\".\n",
    "\n",
    "Intuitively, lane 1 would definitely have the most volume of traffic since all roads would have a minimum of at least 1 lane wheras having 2 to 5 lanes is definitely not a given. Hence this data does make sense as to why the volume of traffic is in chronological order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2986898a",
   "metadata": {},
   "source": [
    "### Direction of Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aebec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "direction_of_travel_groupby = combined_df.groupby(\"direction_of_travel_name\").sum()\n",
    "direction_of_travel_groupby.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f8bf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dims = (12, 10)\n",
    "fig, ax= plt.subplots(figsize=plot_dims)\n",
    "plt.title(\"Total Volume by Travel\")\n",
    "sns.barplot(x=\"direction_of_travel_name\", y=\"total_volume\", data=direction_of_travel_groupby)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a09c91e",
   "metadata": {},
   "source": [
    "Total traffic volume for the direction of travel is the greatest for the South, which makes sense since New York is situated in Northern US and it most people would be more likely to travel south towards Central America "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932148f3",
   "metadata": {},
   "source": [
    "### Sensor Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b718f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"type_of_sensor_name\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed4273c",
   "metadata": {},
   "source": [
    "We mainly have 4 types of sensor in this dataset, they are:\n",
    "\n",
    "1. Inducatance Loop\n",
    "2. Sonic/Acoustic\n",
    "3. Microwave\n",
    "4. Piezoelectric\n",
    "\n",
    "we can take a look at the average volume for each of the sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114079c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = create_years_of_operation_column(combined_df)\n",
    "sensor_groupby = combined_df.groupby(\"type_of_sensor_name\").mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5637bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dims = (12, 10)\n",
    "fig, ax= plt.subplots(figsize=plot_dims)\n",
    "plt.title(\"Average Total Volume by Sensor\")\n",
    "sns.barplot(x=\"type_of_sensor_name\", y=\"total_volume\", data=sensor_groupby)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c224de11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dims = (12, 10)\n",
    "fig, ax= plt.subplots(figsize=plot_dims)\n",
    "plt.title(\"Average Service Year by Sensor\")\n",
    "sns.barplot(x=\"type_of_sensor_name\", y=\"year_of_service\", data=sensor_groupby)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba08003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dims = (12, 10)\n",
    "fig, ax= plt.subplots(figsize=plot_dims)\n",
    "plt.title(\"Count of Sensor Type\")\n",
    "sns.countplot(station_df[\"type_of_sensor_name\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92aba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.groupby(\"type_of_sensor_name\").min()[\"year_of_service\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0d38c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.groupby(\"type_of_sensor_name\").max()[\"year_of_service\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37b7c1e",
   "metadata": {},
   "source": [
    "Based on our analysis above, there are a few interesting takeaways.\n",
    "\n",
    "1. Piezoelectric has the most average total volume among all the sensors even though inductance loop is the most commonly used sensor type. \n",
    "\n",
    "2. Inducatance loop is also the sensor that is the oldest sensor type and also the most commonly used method (inducatance loop). It is still used today and was even still used 55 years ago!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9720bbb",
   "metadata": {},
   "source": [
    "### Longitude and Latitude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b312c0",
   "metadata": {},
   "source": [
    "We will be working with station_df data instead since we only need the station specific information. However, based on my external research, I've noticed some discrepency in the longitude data. Finding longitude between 72.20 and 79.75 brought me to India (Using the current latitude).\n",
    "\n",
    "After experimenting, it seems like the longitude data should be negative. Will be cleaning up the data in this manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7091828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"longitude\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df7267",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "station_df[\"longitude\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb2dd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"latitude\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a566d19a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "station_df[\"latitude\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a614a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df[\"longitude\"] = station_df[\"longitude\"] * -1\n",
    "combined_df[\"longitude\"] = station_df[\"longitude\"] * -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbe6b89",
   "metadata": {},
   "source": [
    "By obtaining the minimum value of both longitude and latitude, we can plot out where each of the station is located in New York City by overlaying a bounding box (similar to object detection in Computer Vision) where we have a matrice of ([min_longitude, max_longitude], [min_latitude, max_latitude]) to encapsulate every single station in our dataset before plotting the stations to ensure we capture all the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df0e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = plt.imread(\"map.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acd5474",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = ((station_df[\"longitude\"].min(), station_df[\"longitude\"].max(),\n",
    "         station_df[\"latitude\"].min(), station_df[\"latitude\"].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09eee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (30, 40))\n",
    "ax.scatter(station_df[\"longitude\"], station_df[\"latitude\"])\n",
    "\n",
    "ax.imshow(maps, extent=bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752a403d",
   "metadata": {},
   "source": [
    "After converting longitude to negative, it seems that we are in the vicinity of New York, however some of the observation stations reside in the water (see Lake Ontario). There is a possibility that we did not manipulate the data correctly or that there might be some issues during data collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b96170",
   "metadata": {},
   "source": [
    "### hpms_sample_identifier & hpms_sample_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17f1616",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.loc[(combined_df[\"hpms_sample_identifier\"].isnull()) & (combined_df[\"hpms_sample_type\"] != \"N\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61716e78",
   "metadata": {},
   "source": [
    "Based on this we can map that the null values for hpms_sample_identifier directly maps to hpms_sample_type. We can choose to keep one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c736d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpms_groupby = combined_df.groupby(\"hpms_sample_identifier\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf314060",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dims = (12, 10)\n",
    "fig, ax= plt.subplots(figsize=plot_dims)\n",
    "plt.title(\"HPMS Sample Identifier\")\n",
    "sns.barplot(x=hpms_groupby.index, y=\"total_volume\", data=hpms_groupby)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5a5a4b",
   "metadata": {},
   "source": [
    "Seems like the specific hpms_sample_identifier has an impact on the total volume of the traffic, hence we will keep the hpms_sample_identifier column over the hpms_sample_type_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110b6938",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.drop(\"hpms_sample_type\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1543eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc54d074",
   "metadata": {},
   "source": [
    "### algorithm_of_vehicle_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01460005",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"algorithm_of_vehicle_classification\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada98b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"algorithm_of_vehicle_classification\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafd91a8",
   "metadata": {},
   "source": [
    "Since the algorithm of vehicle classification column only has F or NaN, we will drop this column as there's two possibility. Either the entire columns have \"F\" as its value ie. does not contribute to prediction, or it could be an unknown algorithm which we do not know. As it comprises of approximate 40% of the data, we will drop the data to be safe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962aca07",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dd1cfa",
   "metadata": {},
   "source": [
    "Some interesting features we can engineer from this dataset could be the total traffic volume in a day. This could be useful when we are trying to determine if there's any seasonality based on months.\n",
    "\n",
    "We can also engineer a new feature where we determine the number of years that the station has been in operation. We can do this by subtracting the year 2015 with the year where the station is established. We should probably do a sanity check that the year_station_discontinued is 0 and the year_of_data is all 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0498770c",
   "metadata": {},
   "source": [
    "### years_of_operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83bd14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"year_station_discontinued\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31274b27",
   "metadata": {},
   "source": [
    "All stations should be operational, hence this data makes sense and we can just drop this since it does not offer any new information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6435d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"year_of_data\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ded6c88",
   "metadata": {},
   "source": [
    "The data only comprises of 2015 data which is why the year of data column only have 15 inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0afa404",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = create_years_of_operation_column(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19088f2",
   "metadata": {},
   "source": [
    "Created a new column indicating the number of years that the observation station is being used and then dropping the year of data and year_station_discontinued column as we don't need it anymore (one unique value only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df68b297",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42e908d",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f19335",
   "metadata": {},
   "source": [
    "Final round of cleaning where we remove any columns with only one unique value as it will not help the model in predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a2e239",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in combined_df.columns:\n",
    "    if len(combined_df[column].unique()) == 1:\n",
    "        combined_df.drop(column, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2829ca",
   "metadata": {},
   "source": [
    "The station_id itself will also not be helpful in predicting traffic volume and we will remove it. We will also remove any locational information as from the previous EDA, it seems like the location data might not be entirely accurate and that might be discrepency in the data collection process (will not be possible to determine the cause of problem without communicating with the data collector).\n",
    "\n",
    "We will also drop the date column as we have already dissected that particular column into other columns such as day_of_data, day_of_week which will capture all the information provided by the date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9a6f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.drop([\"station_id\", \"fips_county_code\", \"longitude\", \"latitude\",\n",
    "                  \"previous_station_id\", \"day_of_data\", \"day_of_week\", \"month_of_data\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75edd85",
   "metadata": {},
   "source": [
    "As we will be predicting the evening rush hour for New York City, we will have to drop columns after 7pm to prevent data leakage as in a real world scenario we will not have information after 8pm when predicting traffic volume for that particular day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80978da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_column_to_drop = []\n",
    "\n",
    "for column in combined_df.columns:\n",
    "    if column.startswith(\"traffic_volume\"):\n",
    "        hour = column.split(\"_\")[-1]\n",
    "        if int(hour) > 1900:\n",
    "            time_column_to_drop.append(column)\n",
    "\n",
    "for column in time_column_to_drop:\n",
    "    combined_df.drop(column, inplace=True, axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d86ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511c25d0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Based on the EDA, some of the interesting insights that we can derive are:\n",
    "\n",
    "1. 3pm to 7pm has the heaviest traffic along with 6am to 10pm (Evening and morning rush hour respectively)\n",
    "2. December have a spike of traffic (could be travelling out of New York to visit their family for the holiday seasons)\n",
    "3. Weekdays typically have heavier traffic compared to weekend (People could be spending more time at home instead of commuting to work.\n",
    "4. Most observation stations are relatively new (built after 2000s) although there is an anomaly in that there is a lot of stations built durign the 1960s (could be any stations that are built before 1960 are automatically binned in 1960)\n",
    "\n",
    "and these are some of the features that was engineered:\n",
    "\n",
    "1. year_of_operation\n",
    "2. max_volume\n",
    "\n",
    "and these are the columns that were dropped:\n",
    "1. method_of_data_retrieval\n",
    "2. method_of_traffic_volume_counting\n",
    "3. year_of_data\n",
    "4. year_station_discontinued\n",
    "5. year_station_established\n",
    "6. record_type\n",
    "7. hpms_sample_type\n",
    "8. station_id\n",
    "9. longitude & latitude\n",
    "10. fips_county_code\n",
    "11. previous_station_id\n",
    "12. day_of_data\n",
    "13. day_of_week\n",
    "14. month_of_data\n",
    "15. algorithm_of_vehicle_classification\n",
    "\n",
    "The next step is to prepare the data to be fed into the model, this is where will have to do one hot encoding for categorical data and scaling for numerical columns before they can be used for any form of model-related activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a1d13e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
